{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6f/2n5c0dxd5_s3mdsw718fp1cm0000gn/T/ipykernel_18152/1565097197.py:10: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'konlpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkonlpy\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkonlpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Okt\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import os \n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import urllib.request\n",
    "import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import konlpy\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt\", filename=\"ratings_total.txt\")\n",
    "\n",
    "reviews_df = pd.read_table('ratings_total.txt', names=['ratings','reviews'])\n",
    "reviews_df = reviews_df.drop_duplicates('reviews')\n",
    "reviews_df = reviews_df.reset_index(drop=True)\n",
    "reviews_df['label'] = reviews_df['ratings'].apply(lambda x : 1 if x>3 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stemming(sentence, tokenizer):\n",
    "    sentence = re.sub(\"[^\\s0-9a-zA-Zㄱ-ㅎㅏ-|가-힣]\", \"\", sentence) # ??\n",
    "    raw_pos_tagged = tokenizer.pos(sentence, stem=True)\n",
    "    sentence_tokenized = []\n",
    "    for token, pos in raw_pos_tagged:\n",
    "        if (len(token) != 1) & (pos in [\"Noun\", \"VerbPrefix\", \"Verb\", \"Adverb\",\"Adjective\", \"Conjuction\",\"KoreanParticle\"]):\n",
    "            sentence_tokenized.append(token)\n",
    "    return sentence_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "tokenized_sentences = []\n",
    "\n",
    "for sentence in tqdm.tqdm(reviews_df['reviews']):\n",
    "    try:\n",
    "        tokenized_sentences.append(tokenize_and_stemming(sentence, okt))\n",
    "    except:\n",
    "        print(\"Error occured at :\", sentence)\n",
    "        tokenized_sentences.append([])\n",
    "        \n",
    "reviews_df['reviews'] = tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/data/ephemeral/temp_data/' \n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "reviews_df.to_csv(data_path + 'data_review_tokenized.csv', encoding='utf-8',index=False)\n",
    "reviews_df = pd.read_csv(data_path+'data_review_tokenized.csv', encoding='utf-8')\n",
    "reviews_df['reviews'] = reviews_df['reviews'].apply(eval)\n",
    "\n",
    "train_x ,test_x, train_y, test_y = model_selection.train_test_split(reviews_df['reviews'], reviews_df['label'],test_size=0.3, random_state=44) # model selectoin은 뭐에요??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 처리\n",
    "tokens = []\n",
    "\n",
    "for token_lst in train_x:\n",
    "    tokens.extend(token_lst)\n",
    "\n",
    "tokens_cnted = Counter(tokens) #Counter는 뭐에요 아 중복되지 않게 세는 거?!\n",
    "\n",
    "n_all_tkn == len(tokens_cnted)\n",
    "n_rare_tkn = 0\n",
    "\n",
    "for t, c in tokens_cnted.items():\n",
    "    if c==1:\n",
    "        n_rare_tkn+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"중복 제거 전체 형태소 개수: {n_all_tkn}\")\n",
    "print(f\"빈도 1인 형태소 수: {n_rare_tkn}\")\n",
    "print(f\"중복 제거 전체 형태소 중 빈도 1인 형태소 비율: {(n_rare_tkn / n_all_tkn)*100:.3f}\")\n",
    "print(f\"중복 포함 전체 형태소 중 빈도 1인 형태소 비율: {(n_rare_tkn / len(tokens))*100:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "빈도가 엄청 높은데 이걸 제외해서 과적합이 나는 거 아닙니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords(text):\n",
    "    find_sw = [k for k,v in tokens_cnted.items() if v==1]\n",
    "    result = [token for token in text if token not in find_sw]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle로 저장하면 좋겠는데요?!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df['token'] = reviews_df['reviews'].apply(stopwords)\n",
    "reviews_df.to_csv(data_path + 'data_reviews_fin.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/data/ephemeral/temp_data/'\n",
    "reviews_df = pd.read_csv(data_path + 'data_reviews_tokenized.csv', encoding='utf-8')\n",
    "# eval 함수 사용 전 형태\n",
    "print(reviews_df.head(1))\n",
    "reviews_df['reviews'] = reviews_df['reviews'].apply(eval)\n",
    "# eval 함수 사용 후\n",
    "print(reviews_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toke -> 불용어 처리 됨\n",
    "reviews_df = pd.read_csv(data_path + 'data_reviews_fin.csv', encoding='utf-8')\n",
    "# eval 깨져서 한 번 더 진행\n",
    "reviews_df['token'] = reviews_df['token'].apply(eval)\n",
    "reviews_df['reviews'] = reviews_df['reviews'].apply(eval)\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getbow(corpus):\n",
    "    bow = {'<PAD>':0, '<UNK>':1}\n",
    "\n",
    "    for line in corpus:\n",
    "        for word in line:\n",
    "            if word not in bow.keys():\n",
    "                bow[word] = len(bow.keys())\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"데이터 전체 수: {len(reviews_df)}\")\n",
    "print(f\"불용어 처리된 길이 수 : {len(reviews_df[reviews_df['reviews'] == reviews_df['toke']])}\")\n",
    "print(\"불용어 수\", 199908-183586)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(reviews_df['toke'], reviews_df['label'], \n",
    "                                                                    test_size=0.2,random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train_x\n",
    "korbow = getbow(corpus=corpus) # 단어 -> 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inkorbow = {v:k for k,v in korbow.items()}\n",
    "len(inkorbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "length_lst = []\n",
    "\n",
    "for num in range(train_x.shape[0]):\n",
    "    length = len(train_x.iloc[num])\n",
    "    length_lst.append(length)\n",
    "    if length > max_length:\n",
    "        max_length=length\n",
    "\n",
    "max_length = 50 # > 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_x, data_y, korbow, max_length):\n",
    "        self.x = data_x\n",
    "        self.y = data_y.values\n",
    "        self.korbow = korbow\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def padding(self, x):\n",
    "        x += ['<PAD>'] * (self.max_length-len(x))\n",
    "        return x\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.padding(self.x[idx])\n",
    "        lst = []\n",
    "        for word in x:\n",
    "            try:\n",
    "                lst.append(korbow[word]) # 단어 -> 숫자\n",
    "            except:\n",
    "                lst.append(korbow['<UNK>'])\n",
    "        x = np.array(lst)\n",
    "        y = self.y[idx]\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loader(data_x, label_y, idx_num, batch_size, train=True):\n",
    "    if train:\n",
    "        train_df = data_x[:idx_num].reset_index(drop=True)\n",
    "        train_y = label_y[:idx_num].reset_index(drop=True)\n",
    "        valid_df = data_x[idx_num:].reset_index(drop=True)\n",
    "        valid_y = label_y[idx_num:].reset_index(drop=True)\n",
    "\n",
    "        train_dataset = MyDataset(train_df, train_y, korbow, max_length)\n",
    "        valid_dataset = MyDataset(valid_df, valid_y, korbow, max_length)\n",
    "        \n",
    "        train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size, shuflle=False) \n",
    "\n",
    "        return train_dataloader, valid_loader\n",
    "    \n",
    "    if not train:\n",
    "        test_df = data_x.reset_index(drop=True)\n",
    "        test_lab = label_y.reset_index(drop=True)\n",
    "\n",
    "        test_ds = MyDataset(test_df, test_lab, korbow, max_length)\n",
    "        test_loader = DataLoader(test_ds, batch_size, shuffle=False)\n",
    "\n",
    "        return test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_CLS(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, ouput_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True) # batch_first는 뭐야?\n",
    "        self.fclayer = nn.Linear(hidden_dim, ouput_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.emb(x)\n",
    "        out, (hidden, _) = self.lstm(embed) # out B * L * H (128, 50, 64) , hidden (1 * B * H)\n",
    "        out = self.fclayer(hidden.squeeze(0))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 2\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 100\n",
    "VOCAB_SIZE = len(korbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_CLS(VOCAB_SIZE, EMB_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    predicted = torch.argmax(output, dim=1)\n",
    "    correct = (predicted==labels).sum().item() #item()은 뭐야\n",
    "    total=labels.size(0)\n",
    "    accuracy=correct/total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에 있듯 약 28000개 정도만 valid로 사용. -> 0.5:0.2:0.3 = train:valid:test 맞나? 아마 \n",
    "train_loader, valid_loader = prepare_loader(train_x,train_y, 127940, 128)\n",
    "test_loader = prepare_loader(test_x, test_y, 0, 64, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    best_loss = float('inf')\n",
    "    early_stop = 30\n",
    "    low_epoch = float('inf')\n",
    "\n",
    "    global train_p, valid_p, train_accs, valid_accs\n",
    "    train_p, valid_p = [], []\n",
    "    train_accs, valid_accs = [], []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        for X,Y in train_loader:\n",
    "            out = model(X)\n",
    "            loss = criterion(out,Y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            train_loss += loss.item() # item()이 모야?\n",
    "            train_correct += accuracy(out, Y) * Y.size(0)\n",
    "            train_total += Y.size(0)\n",
    "\n",
    "        train_acc = train_correct / train_total\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        valid_loss, valid_acc = evaluate(model, valid_loader, criterion, device)\n",
    "        \n",
    "        train_p.append(train_loss) \n",
    "        valid_p.append(valid_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        valid_accs.append(valid_acc)\n",
    "\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{NUM_EPOCHS}')\n",
    "        print(f\"Train Loss: {train_loss:.4f},    Train_Acc: {train_acc:.4f}\")\n",
    "        print(f\"Validation loss {valid_loss:.4f},    Validation Acc: {valid_acc:.4f}\")\n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            print(f\"최고 값 loss {best_loss:.4f} 에서 {valid_loss:.4f} 로 변경.\")\n",
    "            best_loss = valid_loss\n",
    "            low_epoch = epoch\n",
    "            torch.save(model.state_dict(), 'best_model_checkpoint.pth')\n",
    "        else:\n",
    "            if early_stop > 0 and low_epoch + early_stop < epoch+1:\n",
    "                print(\"Early Stop\")\n",
    "                break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_loader, criterion, device):\n",
    "    val_loss, val_corr, val_tota = 0,0,0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no.grad():\n",
    "        for X,Y in valid_loader:\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            predict = model(X)\n",
    "            loss = criterion(predict, Y)\n",
    "\n",
    "            val_loss += loss\n",
    "            val_corr += accuracy(predict, Y) * Y.size(0)\n",
    "            val_tota += Y.size(0)\n",
    "\n",
    "        val_acc = val_corr / val_tota\n",
    "        val_loss /= len(valid_loader)\n",
    "\n",
    "    return val_loss, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_state_dict(torch.load('best_model_checkpoint.pth'))\n",
    "model.to(device)\n",
    "\n",
    "val_loss, val_acc = evaluate(model, valid_loader, criterion, device)\n",
    "\n",
    "print(f'Best valid loss: {val_loss:.4f}')\n",
    "print(f'Best valid acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f'Best test loss: {test_loss:.4f}')\n",
    "print(f'Best test acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dict()\n",
    "result[\"Train Loss\"] = train_p\n",
    "result[\"Valid Loss\"] = valid_p\n",
    "\n",
    "result[\"Train Acc\"] = train_accs\n",
    "result[\"Valid Acc\"] = valid_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train/Valid History\n",
    "\n",
    "plot_from = 0\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"Train/Valid Loss History\", fontsize = 20)\n",
    "plt.plot(\n",
    "    range(0, len(result['Train Loss'][plot_from:])),\n",
    "    result['Train Loss'][plot_from:],\n",
    "    label = 'Train Loss'\n",
    "    )\n",
    "\n",
    "plt.plot(\n",
    "    range(0, len(result['Valid Loss'][plot_from:])),\n",
    "    result['Valid Loss'][plot_from:],\n",
    "    label = 'Valid Loss'\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = {0: '부정', 1:'긍정'}\n",
    "okt = Okt()\n",
    "\n",
    "def predict(text, model, korbow, check):\n",
    "    model.eval()\n",
    "\n",
    "    # 문장 토큰화 \n",
    "    tokens = tokenize_and_stemming(text, okt)\n",
    "    toke = [korbow.get(token, 1) for token in tokens]\n",
    "    input= torch.tensor([toke], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input)\n",
    "\n",
    "    predict = torch.argmax(output, dim=1)\n",
    "    predict_ = check[predict.item()]\n",
    "\n",
    "    return predict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= '아 진짜 이 옷 별로네요'\n",
    "\n",
    "predict(text, model, korbow, check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= '너무 잘 산 것 같아요 감사합니다 잘 쓸게요.'\n",
    "predict(text, model, korbow, check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= '똥똥 잘 맞추는 것 같아 다행이네요.'\n",
    "predict(text, model, korbow, check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= '아 진짜 괜히 삼. 님들은 돈 낭비하지 마셈.'\n",
    "predict(text, model, korbow, check)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
